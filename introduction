\section{\Large Introduction} 
Image retrieval by querying visual contents has been on the agenda of the database, information retrieval, multimedia, and computer vision communities for decades \cite{liu2007survey} \cite{datta2008image}. Search engines like Baidu, Bing or Google perform reasonably well on this task, but crucially rely on textual cues that accompany an image: tags, caption, URL string, adjacent text etc. This fact motivated us to look for associated textual content based cues.  In recent years, deep learning has led to a boost in the quality of visual object recognition in images with fine-grained object labels \cite{simonyan2014very} \cite{lecun2015deep}. Methods like LSDA \cite{hoffman2014lsda} are trained on more than 15,000 classes of ImageNet \cite{deng2009imagenet} (which are mostly leaflevel synsets of WordNet \cite{miller1995wordnet}), and annotate newly seen images with class labels for bounding boxes of objects. Object labels, if recognized, make images easily retrievable for queries with these concepts. However, these labels come with uncertainty. For some images, there is much higher noise in its visual object labels; so querying by visual labels would not work here.\par
Problem: These limitations of text-based search, on one hand, and visual-object search, on the other hand, suggest combining the cues from text and vision for more effective retrieval. Although each side of this combined feature space is incomplete and noisy, the hope is that the combination can improve retrieval quality. Unfortunately, images that show more sophisticated scenes, or emotions evoked on the viewer are still out of reach. The search results would best be retrieved by queries with abstract words (e.g. "environment friendly") or activity words (e.g. "traffic") rather than words that directly correspond to visual objects (e.g. "car" or "bike"). So there is a vocabulary gap, or even concept mismatch, between what users want and express in queries and the visual and textual cues that come directly with an image. This is the key problem addressed in this work.\par
Challenges: The availabilty of a suitable dataset which is not domain specific, consisted of textual content and huge enough for our experiments was one of the most daunting initial challenges. Moreover, we needed to extract utility from a set of images which do not correspond to a particular domain. Since we were trying to enhance a given set of tags, we were essentially required to prove that our set of tags are actually better than any baseline out there. There wasn't any specific section of text relevant to the image, so we also had to check if the text entity is indeed relevant to the image or not. These facts establish the fact that the problem is hard.\par
Approach and Contribution: To bridge the concepts and vocabulary between user queries and image features, we propose an approach that harnesses commonsense knowledge (CSK). Recent advances in automatic knowledge acquisition have produced large collections of CSK: physical (e.g. color or shape) as well as abstract (e.g. abilities) properties of everyday objects (e.g. bike, bird, sofa, etc.), subclass and partwhole relations between objects, activities and their participants, and more \cite{suchanek2007yago} \cite{tandon2015knowlywood} \cite{Liu:2004:CMP:1031314.1031373}. This kind of knowledge allows us to establish relationships between our entities and observable objects or activities in the image. This allows for retrieval of images with generic queries like "big companies". This idea is worked out into a query expansion model where we leverage a knowledge base for automatically expanding the entities with additional related entities. Our model unifies three kinds of features: textual features from the page context of an image, visual features obtained from recognizing fine-grained object classes in an image, and knowledge base features in the form of additional properties of the concepts referred to by entities. The weighing of the different features is crucial for optimal unification.\par
The paperâ€™s contribution can be characterized as follows. We present the model for incorporating CSK into image retrieval. We develop a complete system architecture for this purpose. Our pipeline uses commonsense knowledge to enhance the set of tags for an image by looking at the components of the images in greater detail. We further discuss experiments that compare our approach to state-of-the-art auto tagging engines in various configurations. Our approach substantially improves the tag quality.